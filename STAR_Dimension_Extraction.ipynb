{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Preparing Data:"
      ],
      "metadata": {
        "id": "nGXtrIaj96_D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import relevant packages"
      ],
      "metadata": {
        "id": "IIN1NfMF-M6I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd # Data handling\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "2bsnBj4l-Pfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive (if data is kept on Google Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7n3IiZaKi_Km",
        "outputId": "5637e3f4-ae96-4ade-ffb9-eaeb87e71df8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zyXJ4ULA9w_S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 347
        },
        "outputId": "f50bfb97-e78e-412a-8560-b35a222490ba"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2663400685.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#Import data frame including all texts and their previously extracted embeddings:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/content/drive/MyDrive/STAR Research/all_embeddings.parquet\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, filesystem, filters, **kwargs)\u001b[0m\n\u001b[1;32m    665\u001b[0m     \u001b[0mcheck_dtype_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtype_backend\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 667\u001b[0;31m     return impl.read(\n\u001b[0m\u001b[1;32m    668\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    669\u001b[0m         \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pandas/io/parquet.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, path, columns, filters, use_nullable_dtypes, dtype_backend, storage_options, filesystem, **kwargs)\u001b[0m\n\u001b[1;32m    272\u001b[0m         )\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             pa_table = self.api.parquet.read_table(\n\u001b[0m\u001b[1;32m    275\u001b[0m                 \u001b[0mpath_or_handle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m                 \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36mread_table\u001b[0;34m(source, columns, use_threads, schema, use_pandas_metadata, read_dictionary, memory_map, buffer_size, partitioning, filesystem, filters, use_legacy_dataset, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification)\u001b[0m\n\u001b[1;32m   1791\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1792\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1793\u001b[0;31m         dataset = ParquetDataset(\n\u001b[0m\u001b[1;32m   1794\u001b[0m             \u001b[0msource\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1795\u001b[0m             \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/pyarrow/parquet/core.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, path_or_paths, filesystem, schema, filters, read_dictionary, memory_map, buffer_size, partitioning, ignore_prefixes, pre_buffer, coerce_int96_timestamp_unit, decryption_properties, thrift_string_size_limit, thrift_container_size_limit, page_checksum_verification, use_legacy_dataset)\u001b[0m\n\u001b[1;32m   1357\u001b[0m             \u001b[0mfragment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparquet_format\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_fragment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msingle_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilesystem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1359\u001b[0;31m             self._dataset = ds.FileSystemDataset(\n\u001b[0m\u001b[1;32m   1360\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0mfragment\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mschema\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfragment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphysical_schema\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1361\u001b[0m                 \u001b[0mformat\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparquet_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Import data frame including all texts and their previously extracted embeddings:\n",
        "df = pd.read_parquet(\"/content/drive/MyDrive/STAR Research/all_embeddings.parquet\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic Regression Model with sklearn"
      ],
      "metadata": {
        "id": "NDyni_HS_55l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold"
      ],
      "metadata": {
        "id": "CMmKNBr5_9zL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# For turning categorical variables into a vector\n",
        "le = LabelEncoder()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialises the logistic regression\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "\n",
        "# To standardise embeddings (mean=0, std=1) - with z score\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "F2UlguXbA8wE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Select specfic corpus\n",
        "dataset = df[df['corpus'] == 'Reddit']\n",
        "\n",
        "n = 400# Define the numbers of authors\n",
        "all_authors = dataset['author'].unique() # Get list of all authors\n",
        "\n",
        "#pd.Series turns the list into a table and then we sample from it\n",
        "random_sample = pd.Series(all_authors).sample(n)\n",
        "df_subset = dataset[dataset['author'].isin(random_sample)] # Select the sampled authors\n",
        "\n",
        "# Unravel the embeddings in the 'embedding' column into a matrix\n",
        "X = np.stack(df_subset['embedding'])\n",
        "\n",
        "# fit assigns a unique integer to each author\n",
        "# transform actually converts the strings to a NumPy array with those integers\n",
        "y = le.fit_transform(df_subset['author'])"
      ],
      "metadata": {
        "id": "3nOV1vA2AGc2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weights for embeddings 105 and 921 (out of the 1024 dimensions) are the most frequently occuring in a count of the highest magnitude weight across texts"
      ],
      "metadata": {
        "id": "NZ5EXbABTwhA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reddit 105:\n",
        "reddit_105= pd.read_parquet(\"/content/drive/MyDrive/STAR Research/Reddit 2 author tests/reddit_105.parquet\")\n",
        "# Unravel the embeddings in the 'embedding' column into a matrix\n",
        "X = np.stack(reddit_105['embedding'])\n",
        "\n",
        "# fit assigns a unique integer to each author\n",
        "# transform actually converts the strings to a NumPy array with those integers\n",
        "y = le.fit_transform(reddit_105['author'])"
      ],
      "metadata": {
        "id": "nPl6DnEEKf8p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reddit 921\n",
        "reddit_921=pd.read_parquet(\"/content/drive/MyDrive/STAR Research/Reddit 2 author tests/reddit_921.parquet\")# Unravel the embeddings in the 'embedding' column into a matrix\n",
        "X = np.stack(reddit_921['embedding'])\n",
        "\n",
        "# fit assigns a unique integer to each author\n",
        "# transform actually converts the strings to a NumPy array with those integers\n",
        "y = le.fit_transform(reddit_921['author'])"
      ],
      "metadata": {
        "id": "inxb_WzwLHSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#step_accuracy=[]\n",
        "#steps=100\n",
        "#for _ in range(steps):\n",
        "\n",
        "all_folds_preds = []\n",
        "all_folds_true = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X, y):\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_val, y_val = X[val_idx], y[val_idx]\n",
        "\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    model = LogisticRegression(solver='lbfgs', max_iter=3000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    val_preds=model.predict(X_val)\n",
        "    all_folds_preds.extend(val_preds)\n",
        "    all_folds_true.extend(y_val)\n",
        "  #step_accuracy.append(accuracy_score(all_folds_true, all_folds_preds))\n",
        "#mean_accuracy=np.mean(step_accuracy)\n",
        "#print(\"n:\", n, \"step accuracy:\", step_accuracy)\n",
        "print(\"accuracy:\", accuracy_score(all_folds_true, all_folds_preds))"
      ],
      "metadata": {
        "id": "hVY_upp2BPlm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c195fe2-ff40-49f1-b920-f95ed9016103"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpreting weights:"
      ],
      "metadata": {
        "id": "wnsYVVzlV9yv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#weights = pd. DataFrame(model.coef_)\n",
        "abs_weights=abs(model.coef_)#here rows are authors, columns are dimensions\n",
        "#weights.to_parquet(f'/content/drive/MyDrive/STAR Research/reddit_author_weights_scaled.parquet')\n",
        "#abs_weights.to_parquet(f'/content/drive/MyDrive/STAR Research/reddit_abs_weights_scaled.parquet')"
      ],
      "metadata": {
        "id": "0eiCX2RlWImK",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(abs_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z4Rz7PA5p19O",
        "outputId": "1e8ce538-ca1a-4ede-a8b2-9a2961a64946"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1024)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abs_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R4pWMnwFsp0m",
        "outputId": "0b099af9-0ae5-4677-8189-8d7b409c0e64"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.01034487 0.00439147 0.00151255 ... 0.01406698 0.00473588 0.00112861]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#abs_weights.T - rows are dimensions, columns are authors"
      ],
      "metadata": {
        "id": "gVy_HEdAqhTw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "abs_weights.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f8jpbHebrqZ_",
        "outputId": "7505d68d-7865-4eaa-8ca1-7e8c82a4dcd7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1, 1024)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abs_weights[0])\n",
        "#all dimensions for 1st author"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmiq3cLQr4Sq",
        "outputId": "1a768a28-ca06-4710-af80-0a03ef2914c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.01034487 0.00439147 0.00151255 ... 0.01406698 0.00473588 0.00112861]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Highest magnitude weight across 1024 dimensions:"
      ],
      "metadata": {
        "id": "s2ToThFPh5BB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#iterate over rows in weights (or columns in weights.T since easier to index)\n",
        "#and determine largest magnitude in each weight vector\n",
        "#argmax outputs the index of the highest weight for each author\n",
        "max_weights=(np.argmax(abs_weights, axis=-1, keepdims=True))\n",
        "print(max_weights.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "qcJ6QFk6jbaI",
        "outputId": "9a20a9e8-fdc7-4aaa-84f8-5690bcd88900"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(max_weights)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "avB0099i2ztJ",
        "outputId": "e3d94d17-19c6-41c6-b31a-e786da7a931b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[361]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(abs_weights[0]))\n",
        "print(np.argmax(abs_weights[0]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e38OanhA3GiD",
        "outputId": "ad5bd9ef-bcb3-4feb-b546-8864821e3af7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.017866942386391676\n",
            "361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#see the most frequently occuring values:\n",
        "unique_max, unique_indices, counts=np.unique(max_weights, return_index=True, return_counts=True)\n",
        "np.argsort(counts, stable=True)\n",
        "\n",
        "#index 21 and 215 of max weights = 6 frequency"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6V9HqlcOz0DT",
        "outputId": "ca108802-5a90-45f5-ce58-26ebdc6dd9a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.sort(counts)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bl-ZQyp54qJI",
        "outputId": "2f4bebb6-c794-4166-d605-ac63dcda9a33"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "np.where(counts==6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u1zJUws42yax",
        "outputId": "707efeb5-1ea4-4da0-d96c-ad2ef9d2dfde"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([], dtype=int64),)"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(unique_max[21], unique_max[215]) #weights 21 and 215 most frequently occuring across 400 authors"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "mtRkn-km4_Jm",
        "outputId": "42f8c72f-1e8f-4e04-8c9f-9cf0be47896e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "index 21 is out of bounds for axis 0 with size 1",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3301953581.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munique_max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munique_max\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m215\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#weights 21 and 215 most frequently occuring across 400 authors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m: index 21 is out of bounds for axis 0 with size 1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(abs_weights.T[105])"
      ],
      "metadata": {
        "id": "-CmCnZtQ6NIm",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(abs_weights.T[921])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "x2ZLRi6dPxBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(abs_weights.T[105]))\n",
        "print(np.argmax(abs_weights.T[105]))\n",
        "#author in index 378 has the highest value of 105 weight"
      ],
      "metadata": {
        "id": "zI0Sxivz78lR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(abs_weights.T[921]))\n",
        "print(np.argmax(abs_weights.T[921]))\n",
        "#author in index 214 has the highest value of 921 weight"
      ],
      "metadata": {
        "id": "Cub6a0S48ai0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(abs_weights.T[105]))\n",
        "print(np.argmin(abs_weights.T[105]))\n",
        "#author in index 275 has the lowest value of 105 weight"
      ],
      "metadata": {
        "id": "ex3vkoa28jvE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.min(abs_weights.T[921]))\n",
        "print(np.argmin(abs_weights.T[921]))\n",
        "#author in index 180 has the lowest value of 921 weight"
      ],
      "metadata": {
        "id": "PmQOtwnX8mv6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Knn Classifier:\n"
      ],
      "metadata": {
        "id": "sw5zY8QWNRBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "#from sklearn.metrics import accuracy_score, classification_report - #already imported above\n",
        "\n",
        "knn=KNeighborsClassifier(\n",
        "    n_neighbors=3,\n",
        "    metric='cosine'\n",
        ")\n",
        "\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "predictions=knn.predict(X_val)\n",
        "accuracy=accuracy_score(y_val, predictions)\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4YxsMhluNTxB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(predictions)"
      ],
      "metadata": {
        "id": "ETgQZ-kIPDG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(accuracy)"
      ],
      "metadata": {
        "id": "FAG10GZhROkM"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}