{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da3b75d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3637e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('all_embeddings.parquet')\n",
    "dataset = df[df['corpus'] == 'Reddit']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16acaf2a",
   "metadata": {},
   "source": [
    "# Who are the closest neighbours to each text?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876497df",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.stack(dataset['embedding'])\n",
    "y = np.array(dataset['author'])\n",
    "\n",
    "nbrs = NearestNeighbors(\n",
    "    n_neighbors=2, \n",
    "    metric='cosine', \n",
    "    algorithm='brute'\n",
    "    )\n",
    "\n",
    "nbrs.fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e167df",
   "metadata": {},
   "outputs": [],
   "source": [
    "same_authors = []\n",
    "\n",
    "for i, text in enumerate(X):\n",
    "    print(f\"Nearest neighbour for text {i}, author {y[i]}\")\n",
    "    for j in range(1, len(distances[i])):\n",
    "        neighbour_index = indices[i][j]\n",
    "        distance = distances[i][j]\n",
    "\n",
    "        if y[i] == y[neighbour_index]:\n",
    "            same_authors.append(y[i])\n",
    "        print(f\" {y[neighbour_index]} Distance: {distance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27cbece",
   "metadata": {},
   "source": [
    "# 1. How does KNN perform as you change the number of authors?\n",
    "\n",
    "This will use 3 text as the 'train' set and 1 text as the inference set per author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb3bf15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e86378",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet('all_embeddings.parquet')\n",
    "dataset = df[df['corpus'] == 'Reddit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb618fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_authors = [2, 5, 10, 25, 50, 100, 150, 200, 250, 300, 350, 400]\n",
    "trials = 100\n",
    "\n",
    "results = {\n",
    "    'n_authors': [],\n",
    "    'mean_acc': [],\n",
    "    'std_acc': []\n",
    "}\n",
    "\n",
    "unique_authors = dataset['author'].unique()\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=1,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "for n in n_authors:\n",
    "\n",
    "    current_acc = []\n",
    "\n",
    "    for _ in range(trials):\n",
    "\n",
    "        selected_authors = np.random.choice(unique_authors, size=n, replace=False)\n",
    "        \n",
    "        subset = dataset[dataset['author'].isin(selected_authors)]\n",
    "\n",
    "        X = np.stack(subset['embedding'])\n",
    "        y = np.array(subset['author'])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            stratify=y,\n",
    "                                                            test_size=n)\n",
    "        \n",
    "        # Suppress warnings only for this block\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            knn.fit(X_train, y_train)\n",
    "            predictions = knn.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        current_acc.append(accuracy)\n",
    "\n",
    "    results['n_authors'].append(n)\n",
    "    results['mean_acc'].append(np.mean(current_acc))\n",
    "    results['std_acc'].append(np.std(current_acc))\n",
    "\n",
    "plt.plot(results['n_authors'], results['mean_acc'])\n",
    "plt.xlabel('Number of authors')\n",
    "plt.ylabel('Mean accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "pd.set_option('display.precision', 3)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8760b9a9",
   "metadata": {},
   "source": [
    "# 2. Replicating attribution results\n",
    "\n",
    "In the paper, they use different support levels and different numbers of authors. The authors divide each text into chunks of 512 tokens and then make an embedding for each chunk. We do the same here. We disregard those authors who have less than 9 chunks/embeddings to their name so that we can use 8 support chunks and 1 chunk for testing per author."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79bae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "\n",
    "dataset = pd.read_parquet('reddit_chunked_embeddings_detailed.parquet')\n",
    "author_counts = dataset['author'].value_counts()\n",
    "\n",
    "# Select only those authors who have 9 or more embeddings (8 for train, 1 for test)\n",
    "valid_authors = author_counts[author_counts >= 9].index\n",
    "\n",
    "filtered_df = dataset[dataset['author'].isin(valid_authors)].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2820863",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_authors = [10, 20, 50, 100, 250, 368] # len(unique_authors = 368)\n",
    "trials = 100\n",
    "support = 8 # Support level in Table 7 of paper\n",
    "\n",
    "results = {\n",
    "    'n_authors': [],\n",
    "    'mean_acc': [],\n",
    "    'std_acc': []\n",
    "}\n",
    "\n",
    "unique_authors = filtered_df['author'].unique()\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=1,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "for n in n_authors:\n",
    "\n",
    "    current_acc = []\n",
    "\n",
    "    for _ in range(trials):\n",
    "\n",
    "        selected_authors = np.random.choice(unique_authors, size=n, replace=False)\n",
    "\n",
    "        subset = filtered_df[filtered_df['author'].isin(selected_authors)]\n",
    "\n",
    "        sampled_dataset = subset.groupby('author').sample(n= support + 1) # +1 so that there is enough for 8 train and 1 test\n",
    "\n",
    "        X = np.stack(sampled_dataset['embedding'])\n",
    "        y = np.array(sampled_dataset['author'])\n",
    "\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                            y, \n",
    "                                                            stratify=y,\n",
    "                                                            test_size=n # 1 test per author\n",
    "                                                            )\n",
    "        \n",
    "        # Suppress warnings only for this block\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            knn.fit(X_train, y_train)\n",
    "            predictions = knn.predict(X_test)\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "\n",
    "        current_acc.append(accuracy)\n",
    "    \n",
    "    results['n_authors'].append(n)\n",
    "    results['mean_acc'].append(np.mean(current_acc))\n",
    "    results['std_acc'].append(np.std(current_acc))\n",
    "\n",
    "plt.plot(results['n_authors'], results['mean_acc'])\n",
    "plt.title(f\"Mean accuracy ({support} train chunk, 1 test chunk) over 100 trials\")\n",
    "plt.xlabel('Number of authors')\n",
    "plt.ylabel('Mean accuracy')\n",
    "plt.grid(True)\n",
    "\n",
    "print(f\"Support: {support}\")\n",
    "df = pd.DataFrame(results)\n",
    "pd.set_option('display.precision', 3)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f81cd0",
   "metadata": {},
   "source": [
    "# 3. Removing dimensions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a85f01e",
   "metadata": {},
   "source": [
    "The first approach we use is to remove each dimension one by one and see how it affects accuracy over many trials. Since we have to loop over 1,024 dimensions, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f6f66e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.ticker import FuncFormatter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7bef0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_authors = filtered_df['author'].unique()\n",
    "n_authors = len(unique_authors)\n",
    "trials = 20\n",
    "support = 8\n",
    "\n",
    "knn = KNeighborsClassifier(\n",
    "    n_neighbors=1,\n",
    "    metric='cosine'\n",
    ")\n",
    "\n",
    "results = {\n",
    "    'deleted_dim': [],\n",
    "    'acc': [],\n",
    "    'trial_id': []\n",
    "}\n",
    "\n",
    "baseline_acc = []\n",
    "\n",
    "for trial_id in range(trials):\n",
    "\n",
    "    # Sample texts from each author\n",
    "    sampled_dataset = filtered_df.groupby('author').sample(n = support + 1) # +1 so that there is enough for 8 train and 1 test\n",
    "\n",
    "    X = np.stack(sampled_dataset['embedding'])\n",
    "    y = np.array(sampled_dataset['author'])\n",
    "\n",
    "    # Assign 1 text as the test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, \n",
    "                                                        y, \n",
    "                                                        stratify=y,\n",
    "                                                        test_size=n_authors\n",
    "                                                        )\n",
    "    \n",
    "    knn.fit(X_train, y_train)\n",
    "    predictions = knn.predict(X_test)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        accuracy = accuracy_score(y_test, predictions)\n",
    "    baseline_acc.append(accuracy)\n",
    "    \n",
    "    dim_acc = []\n",
    "    \n",
    "    # Loop over columns of matrix \n",
    "    for dim in range(X_train.shape[1]):\n",
    "        \n",
    "        X_train_del = np.delete(X_train, dim, axis=1)\n",
    "        X_test_del = np.delete(X_test, dim, axis=1)\n",
    "\n",
    "        knn.fit(X_train_del, y_train)\n",
    "        predictions = knn.predict(X_test_del)\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\")\n",
    "            accuracy = accuracy_score(y_test, predictions)\n",
    "        \n",
    "        results['deleted_dim'].append(dim)\n",
    "        results['acc'].append(accuracy)\n",
    "        results['trial_id'].append(1)\n",
    "\n",
    "results = pd.DataFrame(results)\n",
    "mean_baseline_acc = np.mean(baseline_acc)\n",
    "\n",
    "dim_stats = results.groupby('deleted_dim')['acc'].agg(['mean', 'std']).reset_index()\n",
    "print(\"Dimensions for which accuracy dropped when removed:\")\n",
    "print(dim_stats.sort_values('mean', ascending=True).head(10))\n",
    "\n",
    "def to_percent(y, position):\n",
    "    return f\"{y * 100:.2f}%\"\n",
    "\n",
    "formatter = FuncFormatter(to_percent)\n",
    "plt.gca().yaxis.set_major_formatter(formatter)\n",
    "\n",
    "plt.plot(dim_stats['deleted_dim'], dim_stats['mean'])\n",
    "plt.title(f'Mean Accuracy Over {trials} Trials After Removing One Dimension')\n",
    "plt.xlabel('Index of Dimension Removed')\n",
    "plt.ylabel('Mean Accuracy')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.axhline(y = mean_baseline_acc, color='r', linestyle='--', label='Baseline (no dimensions removed)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af22a12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_rel\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(results)\n",
    "\n",
    "# Create a list to store stats per dimension\n",
    "significance_data = []\n",
    "\n",
    "# Loop through every unique dimension\n",
    "for dim in df['deleted_dim'].unique():\n",
    "    # Get the list of accuracies for this specific dimension\n",
    "    dim_accs = df[df['deleted_dim'] == dim]['acc'].values\n",
    "    \n",
    "    # Compare against the baseline list (must be same length!)\n",
    "    # ttest_rel checks: \"Is the mean difference significantly different from zero?\"\n",
    "    t_stat, p_val = ttest_rel(baseline_acc, dim_accs)\n",
    "    \n",
    "    # Calculate average drop\n",
    "    mean_drop = np.mean(baseline_acc) - np.mean(dim_accs)\n",
    "    \n",
    "    significance_data.append({\n",
    "        'dim': dim,\n",
    "        'p_value': p_val,\n",
    "        'mean_diff_from_baseline': mean_drop,\n",
    "    })\n",
    "\n",
    "stats_df = pd.DataFrame(significance_data)\n",
    "\n",
    "# Show dimensions that significantly HURT accuracy (Positive drop, Low P-value)\n",
    "print(\"--- Significantly Important Dimensions ---\")\n",
    "important_dims = stats_df.sort_values('mean_diff_from_baseline', ascending=False)\n",
    "\n",
    "print(important_dims.head(20))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
