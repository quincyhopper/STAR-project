{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Setup"
      ],
      "metadata": {
        "id": "AS5W5mjVOYZh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UfSNTsXIsLC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd # Data handling\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report\n",
        "from sklearn.model_selection import StratifiedKFold"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount drive (if data is kept on Google Drive)\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0ChdZO9SOJpL",
        "outputId": "ec9019cd-d9a8-4dae-c5be-3d5882628f3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For turning categorical variables into a vector\n",
        "le = LabelEncoder()\n",
        "\n",
        "skf = StratifiedKFold(n_splits=4, shuffle=True, random_state=42)\n",
        "\n",
        "# Initialises the logistic regression\n",
        "model = LogisticRegression(solver='lbfgs')\n",
        "\n",
        "# To standardise embeddings (mean=0, std=1) - with z score\n",
        "scaler = StandardScaler()"
      ],
      "metadata": {
        "id": "G7QLkgb4OPde"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Import df and init x and y"
      ],
      "metadata": {
        "id": "RDNBK9w5Ocna"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#reddit 105:\n",
        "reddit_105= pd.read_parquet(\"/content/drive/MyDrive/STAR Research/Reddit 2 author tests/reddit_105.parquet\")\n",
        "# Unravel the embeddings in the 'embedding' column into a matrix\n",
        "X = np.stack(reddit_105['embedding'])\n",
        "\n",
        "# fit assigns a unique integer to each author\n",
        "# transform actually converts the strings to a NumPy array with those integers\n",
        "y = le.fit_transform(reddit_105['author'])"
      ],
      "metadata": {
        "id": "Rbt-vjX1OTXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#reddit 921:\n",
        "reddit_921= pd.read_parquet(\"/content/drive/MyDrive/STAR Research/Reddit 2 author tests/reddit_921.parquet\")\n",
        "# Unravel the embeddings in the 'embedding' column into a matrix\n",
        "X = np.stack(reddit_921['embedding'])\n",
        "\n",
        "# fit assigns a unique integer to each author\n",
        "# transform actually converts the strings to a NumPy array with those integers\n",
        "y = le.fit_transform(reddit_921['author'])"
      ],
      "metadata": {
        "id": "14aEJtAQRLKX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#no quote reddit 921:\n",
        "nq_reddit_921= pd.read_parquet(\"/content/drive/MyDrive/reddit_921_plus_emb.parquet\")\n",
        "# Unravel the embeddings in the 'embedding' column into a matrix\n",
        "X = np.stack(nq_reddit_921['embedding'])\n",
        "\n",
        "# fit assigns a unique integer to each author\n",
        "# transform actually converts the strings to a NumPy array with those integers\n",
        "y = le.fit_transform(nq_reddit_921['author'])"
      ],
      "metadata": {
        "id": "0V3pTw_zT5rZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#logreg"
      ],
      "metadata": {
        "id": "LOwr3XXvOlCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_folds_preds = []\n",
        "all_folds_true = []\n",
        "\n",
        "for train_idx, val_idx in skf.split(X, y):\n",
        "    X_train, y_train = X[train_idx], y[train_idx]\n",
        "    X_val, y_val = X[val_idx], y[val_idx]\n",
        "\n",
        "    X_train = scaler.fit_transform(X_train)\n",
        "    X_val = scaler.transform(X_val)\n",
        "\n",
        "    model = LogisticRegression(solver='lbfgs', max_iter=3000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    val_preds=model.predict(X_val)\n",
        "    all_folds_preds.extend(val_preds)\n",
        "    all_folds_true.extend(y_val)\n",
        "print(\"accuracy:\", accuracy_score(all_folds_true, all_folds_preds))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J8PuDSCkOWSf",
        "outputId": "55c09ca5-f879-4861-d733-7b400b217174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Interpret weights"
      ],
      "metadata": {
        "id": "YrRUh7rWOwuw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "abs_weights=abs(model.coef_)#here rows are authors, columns are dimensions\n",
        "abs_weights_df=pd.DataFrame(abs_weights)\n",
        "abs_weights_df.to_parquet(f'/content/drive/MyDrive/STAR Research/reddit_921_nq_weights.parquet')"
      ],
      "metadata": {
        "id": "Q8jyCiE7OyeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.max(abs_weights))\n",
        "print(np.argmax(abs_weights))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TnZuxUK9QaFg",
        "outputId": "aeb0ee77-263f-4c30-865c-8d3c63accdc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.017888874830305247\n",
            "361\n"
          ]
        }
      ]
    }
  ]
}